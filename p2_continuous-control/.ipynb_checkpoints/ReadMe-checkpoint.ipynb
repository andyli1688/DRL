{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Details\n",
    "\n",
    "For this project, we will train an agent to navigate the Reacher environment.\n",
    "\n",
    "![SegmentLocal](images/reacher.gif \"segment\")\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Details\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  \n",
    "At each time step, it has 33 variables corresponding to:\n",
    "- Position\n",
    "- Rotation\n",
    "- Velocity\n",
    "- Angular Velocities of the arm\n",
    "\n",
    "Each action is a vector with four numbers, corresponding to torque applicable to two joints. \n",
    "\n",
    "A sample output is shown as below.\n",
    "\n",
    "Number of agents: 1\n",
    "\n",
    "Size of each action: 4\n",
    "\n",
    "There are 1 agents. Each observes a state with length: 33\n",
    "\n",
    "The state for the first agent looks like: \n",
    "[ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
    " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
    "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
    "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
    "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
    "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
    "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
    "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
    " -1.68164849e-01]\n",
    " \n",
    "## Every entry in the action vector should be a number between -1 and 1.\n",
    "\n",
    "Solving Criteria: \n",
    "\n",
    "Option 1: Solve the First Version\n",
    "The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over `100` consecutive episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Set up python environment to run the code in this repository, follow the instructions below.\n",
    "\n",
    "1. Create (and activate) a new environment with Python 3.6.\n",
    "\t- __Mac OSX__: \n",
    "\t```bash\n",
    "\tconda create --name drlnd python=3.6\n",
    "\tconda activate drlnd\n",
    "\t```\n",
    "2. Create a DRLND folder and Perform a minimal install of OpenAI gym.  \n",
    "\t```bash\n",
    "    cd DRLND\n",
    "    git clone https://github.com/openai/gym.git\n",
    "    cd gym\n",
    "    pip install -e .\n",
    "    cd ..\n",
    "\t```\n",
    "3. Clone the DRLND repository, and navigate to the `python/` folder.  Then, install several dependencies.\n",
    "    ```bash\n",
    "    git clone https://github.com/udacity/deep-reinforcement-learning.git\n",
    "    cd deep-reinforcement-learning/python\n",
    "    pip install .\n",
    "    ```\n",
    "\n",
    "4. Create an [IPython kernel](http://ipython.readthedocs.io/en/stable/install/kernel_install.html) for the `drlnd` environment.  \n",
    "```bash\n",
    "python -m ipykernel install --user --name drlnd --display-name \"drlnd\"\n",
    "```\n",
    "\n",
    "5. Download the Unity Environment \n",
    "   - __Mac OSX__: [Reacher.app.zip](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher.app.zip)\n",
    "\n",
    "\n",
    "6. Place the unzipped file (Banana.app) in the p2_continuous-control/ folder in the DRLND GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Run the code in this repository, follow the instructions below.\n",
    "\n",
    "1. Open a jupyter Notebook in the DRLND environment\n",
    "```bash\n",
    "conda activate drlnd\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "2. Navigating to p1_navigation/ folder and open the Navigation.ipynb\n",
    "\n",
    "\n",
    "3. Before running code in the notebook, change the kernel to match the `drlnd` environment by using the drop-down `Kernel` menu. \n",
    "![SegmentLocal](images/kernel.png)\n",
    "\n",
    "\n",
    "4. Use 'Shift-Enter' to run codes line by line and watch how the agent gets trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
