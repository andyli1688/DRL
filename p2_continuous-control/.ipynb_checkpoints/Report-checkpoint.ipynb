{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Implementation\n",
    "## Learning Algorithm\n",
    "* ### Algorithm - Deep Deterministic Policy Gradients (DDPG)Implementation\n",
    "This project implements an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces called [Deep Deterministic Policy Gradients](https://arxiv.org/abs/1509.02971). \n",
    "\n",
    "![Deep Q Network](images/DDPG.png)\n",
    "* ### Hyperparameters\n",
    "    * Replay buffer size\n",
    "      ```bash\n",
    "      list BUFFER_SIZE = int(1e5)   \n",
    "      ```\n",
    "    * Minibatch size\n",
    "      ```bash\n",
    "      BATCH_SIZE = 128             \n",
    "      ```\n",
    "    * Discount factor\n",
    "    ```bash\n",
    "    GAMMA = 0.99                  \n",
    "    ```\n",
    "    *  For soft update of target parameters\n",
    "    ```bash\n",
    "    TAU = 1e-3                    \n",
    "    ```\n",
    "    * Learning rate of the Actor \n",
    "    ```\n",
    "    LR_ACTOR = 2e-4              \n",
    "    ```\n",
    "    * Learning rate of the Critic\n",
    "    ```\n",
    "    LR_CRITIC = 2e-4           \n",
    "    ```\n",
    "    * Learning rate of the Critic\n",
    "    ```\n",
    "    LR_CRITIC = 2e-4           \n",
    "    ```\n",
    "    * L2 weight decay\n",
    "    ```\n",
    "    WEIGHT_DECAY = 0        \n",
    "    ```\n",
    "    * Learning timestep interval\n",
    "    ```\n",
    "    LEARN_EVERY = 8        \n",
    "    ```\n",
    "    * Number of Learning Passes\n",
    "    ```\n",
    "    LEARN_NUM = 4       \n",
    "    ```\n",
    "    * OU Process\n",
    "    ```\n",
    "    mu=0.\n",
    "    theta=0.15\n",
    "    sigma=0.08\n",
    "    ```\n",
    "* ### Accelerate the trainings\n",
    "    * Adding a check in the step function to only learn once every 5-10 steps, and then when it is time to learn, call the learn function several times (say, 4-8 times). \n",
    "    \n",
    "    * in ddpg_agent.py,\n",
    "      Learn, if enough samples are available in memory\n",
    "    ```\n",
    "        if len(self.memory) > BATCH_SIZE and timestep % LEARN_EVERY ==0:\n",
    "            for _ in range(LEARN_NUM):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "    ```\n",
    "    * in agent.step, add timestep \n",
    "    ```\n",
    "    def step(self, state, action, reward, next_state, done, timestep)\n",
    "    ```\n",
    "    * in the main ipydb file,\n",
    "      ```\n",
    "      agent.step(state, action, reward, next_state, done,t)\n",
    "      t = t+1\n",
    "      ```\n",
    "   \n",
    "* ### Neural Network Architecture\n",
    "    The Neural Network will take states as inputs and output actions.\n",
    "    * Actor\n",
    "    ```\n",
    "    fc1_units=256, fc2_units = 128\n",
    "    BatchNorm1d and ReLu are applied\n",
    "    The final output is generated through Tanh\n",
    "    ```\n",
    "    * Critic\n",
    "    ```\n",
    "    fc1_units=256, fc2_units = 128\n",
    "    BatchNorm1d and ReLu are applied\n",
    "    ```\n",
    "* ### Clip the action between -1 and 1\n",
    "    return np.clip(action, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards\n",
    "\n",
    "Running DDPG with above hyperparameters and Neural Network Architecture, the agent is able to receive an average reward (over 100 episodes) of at least `+30` after `187` episodes!\n",
    "\n",
    "![Epsode Solution](images/episode_solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "* ### Read paper to determine performance of various deep RL algorithms on continuous control tasks\n",
    "    * Implement REINFORCE, TNPG, RWR, REPS, TRPO, CEM, CMA-ES and DDPG,\n",
    "    * [Deep Deterministic Policy Gradients](https://arxiv.org/abs/1604.06778)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
