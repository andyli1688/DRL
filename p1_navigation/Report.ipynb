{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Implementation\n",
    "## Learning Algorithm\n",
    "* ### Algorithm - DQN Implementation\n",
    "This project implements a *Value Based* method called [Deep Q-Networks](https://deepmind.com/research/dqn/). \n",
    "\n",
    "    Deep Q Learning combines 2 approaches :\n",
    "    - A Reinforcement Learning method called [Q Learning](https://en.wikipedia.org/wiki/Q-learning) (aka SARSA max)\n",
    "    - A Deep Neural Network to learn a Q-table approximation (action-values)\n",
    "\n",
    "Especially, this implementation includes the 2 major training improvements by [Deepmind](https://deepmind.com) and described in their [Nature publication : \"Human-level control through deep reinforcement learning (2015)\"](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n",
    "    - Experience Replay \n",
    "    - Fixed Q Targets\n",
    "\n",
    "> Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as Q) function20. This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations\n",
    "between the action-values and the target values .\n",
    "We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. Second, we used an iterative update that adjusts the action-values towards target values that are only periodically updated, thereby reducing correlations with the target.\n",
    "\n",
    "![Deep Q Network](./DQN.png)\n",
    "* ### Hyperparameters\n",
    "    * Replay buffer size\n",
    "      ```bash\n",
    "      list BUFFER_SIZE = int(1e5)   \n",
    "      ```\n",
    "    * Minibatch size\n",
    "      ```bash\n",
    "      BATCH_SIZE = 64             \n",
    "      ```\n",
    "    * Discount factor\n",
    "    ```bash\n",
    "    GAMMA = 0.99                  \n",
    "    ```\n",
    "    *  For soft update of target parameters\n",
    "    ```bash\n",
    "    TAU = 1e-3                    \n",
    "    ```\n",
    "    * Learning rate \n",
    "    ```\n",
    "    LR = 5e-4               \n",
    "    ```\n",
    "    * How often to update the network\n",
    "    ```\n",
    "    UPDATE_EVERY = 4        \n",
    "    ```\n",
    "    *  Number of nodes in first hidden layer\n",
    "    ```bash\n",
    "    fc1_units (int)         \n",
    "    ```\n",
    "    * Number of nodes in second hidden layer\n",
    "    ```bash\n",
    "    fc2_units (int)         \n",
    "    ```\n",
    "    * Start epsilon value\n",
    "    ```bash\n",
    "    eps_start = 1.0\n",
    "    ```\n",
    "    \n",
    "    * End epsilon value\n",
    "    ```bash\n",
    "    eps_end = .01\n",
    "    ```\n",
    "    \n",
    "    * Epsilon Decay Rate\n",
    "    ```bash\n",
    "    eps_decay = .995\n",
    "    ```\n",
    "* ### Neural Network Architecture\n",
    "    The Neural Network will take states as inputs and output actions.\n",
    "    ![Neural Network Structure](./NN_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards\n",
    "\n",
    "Running DQN with above hyperparameters and Neural Network Architecture, the agent is able to receive an average reward (over 100 episodes) of at least `+13` after `511` episodes!\n",
    "\n",
    "![Epsode Solution](./Epsode_solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "* ### Algorithm Updates\n",
    "    * Implement Double DQN \n",
    "    * Implement Dueling DQN\n",
    "* ### Dealing with Visualized Environment\n",
    "    * Instead of using banana.app, use visualbanana.app\n",
    "    * Examine the state space \n",
    "      ```bash\n",
    "      change\n",
    "      state = env_info.vector_observations[0]\n",
    "      to\n",
    "      state = env_info.visual_observations[0]\n",
    "      ```\n",
    "    * Design the suitable neural network architecture\n",
    "    ```bash\n",
    "      1. Examine the size of the state.\n",
    "         It becomes (1,84,84,3), a 3D state with 3 channels.\n",
    "      2. Build a Convolutional Neural Network to handle the pixel inputs.\n",
    "         a. conv1 = nn.Conv3d(3, 10, (1, 3, 3), stride = (1, 3, 3))\n",
    "         b. conv2 = nn.Conv3d(10, 128, (1,3, 3), stride = (1, 3, 3))\n",
    "         c. pooling = nn.MaxPool3d((2,2,2), stride = (2,2,2) - reduce the size\n",
    "         d. fc1 = nn.Linear(?, 64)\n",
    "         e. fc2 = nn.Linear(64, 4)\n",
    "         f. Relu as activation function\n",
    "      3. Train the agent and watch the scores and check how many episodes it takes to solve the environment!\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
