{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Implementation\n",
    "## Learning Algorithm\n",
    "* ### Algorithm - Deep Deterministic Policy Gradients (DDPG)Implementation\n",
    "This project implements an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces called [Deep Deterministic Policy Gradients](https://arxiv.org/abs/1509.02971). \n",
    "\n",
    "![Deep Q Network](images/DDPG.png)\n",
    "* ### Hyperparameters\n",
    "    * Replay buffer size\n",
    "      ```bash\n",
    "      list BUFFER_SIZE = int(1e5)   \n",
    "      ```\n",
    "    * Minibatch size\n",
    "      ```bash\n",
    "      BATCH_SIZE = 250            \n",
    "      ```\n",
    "    * Discount factor\n",
    "    ```bash\n",
    "    GAMMA = 0.99                  \n",
    "    ```\n",
    "    *  For soft update of target parameters\n",
    "    ```bash\n",
    "    TAU = 1e-3                    \n",
    "    ```\n",
    "    * Learning rate of the Actor \n",
    "    ```\n",
    "    LR_ACTOR = 1e-4              \n",
    "    ```\n",
    "    * Learning rate of the Critic\n",
    "    ```\n",
    "    LR_CRITIC = 1e-3           \n",
    "    ```\n",
    "    * L2 weight decay\n",
    "    ```\n",
    "    WEIGHT_DECAY = 0        \n",
    "    ```\n",
    "    * OU Process\n",
    "    ```\n",
    "    mu=0.\n",
    "    theta=0.15\n",
    "    sigma=0.2\n",
    "    ```\n",
    "* ### Accelerate the trainings\n",
    "    \n",
    "    * in ddpg_agent.py,\n",
    "      Learn, if enough samples are available in memory\n",
    "    ```\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "    ```\n",
    "   \n",
    "* ### Neural Network Architecture\n",
    "    The Neural Network will take states as inputs and output actions.\n",
    "    * Actor\n",
    "    ```\n",
    "    fc1_units=200, fc2_units = 150\n",
    "    ReLu is applied\n",
    "    The final output is generated through Tanh\n",
    "    ```\n",
    "    * Critic\n",
    "    ```\n",
    "    For multiple agents, we define the inputs for all agents\n",
    "    self.fcs1 = nn.Linear((state_size+action_size) * num_agents, fcs1_units)\n",
    "    fc1_units=200, fc2_units = 150\n",
    "    ReLu is applied\n",
    "    ```\n",
    "* ### Setup for Multiple Agents\n",
    "    * Define Memory to save teh replaybuffer for all agents\n",
    "       ```\n",
    "       sharedBuffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "       ```\n",
    "    * Initialize multiple agents\n",
    "       ```\n",
    "       self.agents = [DDPGAgent(state_size,action_size,random_seed) for x in range(num_agents)]\n",
    "       ```\n",
    "    * Add current (states, actions, rewards, next_states, dones) into sharedBuffer\n",
    "       ```\n",
    "       sharedBuffer.add(states, actions, rewards, next_states, dones)\n",
    "       ```\n",
    "    * Actions based on individual agent's action\n",
    "       ```\n",
    "       for index, agent in enumerate(self.agents):\n",
    "            actions[index, :] = agent.act(states[index], add_noise)\n",
    "       ```\n",
    "\n",
    " \n",
    "                \n",
    "* ### Clip the action between -1 and 1\n",
    "    return np.clip(action, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards\n",
    "\n",
    "Running DDPG with above hyperparameters and Neural Network Architecture, the agent is able to receive an average reward (over 100 episodes) of at least `+0.5` after `1563` episodes!\n",
    "\n",
    "![Epsode Solution](images/Episode_solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Future Work\n",
    "* ### Read paper to determine performance of various deep RL algorithms on continuous control tasks\n",
    "    * Implement REINFORCE, TNPG, RWR, REPS, TRPO, CEM, CMA-ES and DDPG,\n",
    "    * [Deep Deterministic Policy Gradients](https://arxiv.org/abs/1604.06778)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
